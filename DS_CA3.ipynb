{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c1cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "726160c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession as sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84270691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>pre { white-space: pre !important; }</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4637af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = sess.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41225430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-ICMKJ16:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2d356853400>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c671f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', 'true').csv('stocks.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fa52081",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header', 'true').csv('stocks.csv', inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "182ddee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36f2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+---------+\n",
      "|              Open|             Close|   Volume|\n",
      "+------------------+------------------+---------+\n",
      "|        213.429998|        214.009998|123432400|\n",
      "|        214.599998|        214.379993|150476200|\n",
      "|        214.379993|        210.969995|138040000|\n",
      "|            211.75|            210.58|119282800|\n",
      "|        210.299994|211.98000499999998|111902700|\n",
      "|212.79999700000002|210.11000299999998|115557400|\n",
      "|209.18999499999998|        207.720001|148614900|\n",
      "|        207.870005|        210.650002|151473000|\n",
      "|210.11000299999998|            209.43|108223500|\n",
      "|210.92999500000002|            205.93|148516900|\n",
      "|        208.330002|        215.039995|182501900|\n",
      "|        214.910006|            211.73|153038200|\n",
      "|        212.079994|        208.069996|152038600|\n",
      "|206.78000600000001|            197.75|220441900|\n",
      "|202.51000200000001|        203.070002|266424900|\n",
      "|205.95000100000001|        205.940001|466777500|\n",
      "|        206.849995|        207.880005|430642100|\n",
      "|        204.930004|        199.289995|293375600|\n",
      "|        201.079996|        192.060003|311488100|\n",
      "|192.36999699999998|        194.729998|187469100|\n",
      "+------------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_spx = df.filter(df['Close'] < 500)\n",
    "\n",
    "# Select the required columns\n",
    "selected_columns = ['Open', 'Close', 'Volume']\n",
    "filtered_spx = filtered_spx.select(*selected_columns)\n",
    "\n",
    "# Show the resulting dataframe\n",
    "filtered_spx.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69cd2462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+---------+\n",
      "|              Open|     Close|   Volume|\n",
      "+------------------+----------+---------+\n",
      "|206.78000600000001|    197.75|220441900|\n",
      "|        204.930004|199.289995|293375600|\n",
      "|        201.079996|192.060003|311488100|\n",
      "+------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_spx2 = df.filter((df['Open'] > 200) & (df['Close'] < 200))\n",
    "\n",
    "# Select the required columns\n",
    "selected_columns = ['Open', 'Close', 'Volume']\n",
    "filtered_spx2 = filtered_spx2.select(*selected_columns)\n",
    "\n",
    "# Show the resulting dataframe\n",
    "filtered_spx2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb8846d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+----+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|2010|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|2010|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|2010|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|2010|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|2010|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|2010|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|2010|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|2010|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|2010|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|2010|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|2010|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|2010|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|2010|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|2010|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|2010|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have the 'spx' dataframe loaded\n",
    "\n",
    "from pyspark.sql.functions import year, col\n",
    "\n",
    "# Add a new column called 'Year' with the extracted year\n",
    "df_with_year = df.withColumn('Year', year(col('Date')))\n",
    "\n",
    "# Show the resulting dataframe with the new 'Year' column\n",
    "df_with_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cff0643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|year|minVolume|\n",
      "+----+---------+\n",
      "|2015| 13046400|\n",
      "|2013| 41888700|\n",
      "|2014| 14479600|\n",
      "|2012| 43938300|\n",
      "|2016| 11475900|\n",
      "|2010| 39373600|\n",
      "|2011| 44915500|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "min_volumes_by_year = df_with_year.groupBy(\"year\").agg(min(\"volume\").alias(\"minVolume\"))\n",
    "min_volumes_by_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d0d4110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|year|month|            maxLow|\n",
      "+----+-----+------------------+\n",
      "|2012|   10|        665.550026|\n",
      "|2010|    7|        260.300003|\n",
      "|2010|   12|        325.099991|\n",
      "|2015|    2|        131.169998|\n",
      "|2014|    4|        589.799988|\n",
      "|2015|   12|        117.809998|\n",
      "|2016|    7|            103.68|\n",
      "|2016|   11|        111.400002|\n",
      "|2012|    8| 673.5400089999999|\n",
      "|2013|    2|473.24997699999994|\n",
      "|2012|    4| 626.0000150000001|\n",
      "|2012|   12|        585.500023|\n",
      "|2014|   10|        107.209999|\n",
      "|2016|    5|             99.25|\n",
      "|2014|   12|        115.290001|\n",
      "|2013|    9|        503.479988|\n",
      "|2013|   10|        525.110016|\n",
      "|2014|    5|        628.900002|\n",
      "|2016|    2|         96.650002|\n",
      "|2013|   12| 566.4100269999999|\n",
      "+----+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, max\n",
    "\n",
    "# Create a Spark session\n",
    "# Assuming you already have the DataFrame 'df' with the 'year' and 'month' columns\n",
    "# (as calculated in the previous steps)\n",
    "\n",
    "# Calculate the highest low price for each year and month\n",
    "max_low_by_year_month = df.groupBy(year(\"Date\").alias(\"year\"), month(\"Date\").alias(\"month\")) \\\n",
    "    .agg(max(\"Low\").alias(\"maxLow\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "max_low_by_year_month.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "635f13be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+\n",
      "|mean_high_price  |stddev_high_price |\n",
      "+-----------------+------------------+\n",
      "|315.9112880164581|186.89817686485767|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "# Create a Spark session\n",
    "\n",
    "\n",
    "# Assuming you already have the DataFrame 'df' with the 'highest' column\n",
    "# (as calculated in the previous steps)\n",
    "\n",
    "# Calculate mean and standard deviation for the 'highest' column\n",
    "result = df.agg(mean(\"High\").alias(\"mean_high_price\"), stddev(\"High\").alias(\"stddev_high_price\"))\n",
    "\n",
    "# Show the result with two decimal places\n",
    "result.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
